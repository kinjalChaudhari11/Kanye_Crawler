{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ6GTW0ehidu"
      },
      "source": [
        "Assignment can be run by running the entire Jupyter Notebook in the Google Colab environment after uploading all the files.\n",
        "\n",
        "Code based on https://blog.devgenius.io/big-data-processing-with-hadoop-and-spark-in-python-on-colab-bff24d85782f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Upload the following files to Google Colab\n",
        "# - HW2.ipynb\n",
        "# - inverted_index.py\n",
        "# - mapper.py\n",
        "# - reducer.py\n",
        "# - 3_tfidf.py\n",
        "# - 4_jm_smoothing.py\n",
        "# - 5_crawler.py\n",
        "# - 5_tfidf.py\n",
        "# - 6_cv_k.py\n",
        "\n",
        "# Then run this cell to change their modes to execution\n",
        "!chmod u+rwx /content/inverted_index.py\n",
        "!chmod u+rwx /content/mapper.py\n",
        "!chmod u+rwx /content/reducer.py\n",
        "!chmod u+rwx /content/3_tfidf.py\n",
        "!chmod u+rwx /content/4_jm_smoothing.py\n",
        "!chmod u+rwx /content/5_crawler.py\n",
        "!chmod u+rwx /content/5_tfidf.py\n",
        "!chmod u+rwx /content/6_cv_k.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBmaZ2C_y_Wr"
      },
      "source": [
        "# Java Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zVSeLgaR4jbW"
      },
      "outputs": [],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V8gDfy9L45Ia"
      },
      "outputs": [],
      "source": [
        "# Create Java home variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Uj23u2Eu1F"
      },
      "source": [
        "# Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Hadoop\n",
        "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decompress the Hadoop tar file\n",
        "!tar -xzvf hadoop-3.3.0.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tar -xzvf hadoop-3.3.0.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BM_4krMLD6bQ"
      },
      "outputs": [],
      "source": [
        "# Copy Hadoop directory to user/local\n",
        "!cp -r hadoop-3.3.0/ /usr/local/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Hadoop from /usr/local\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create input folder (test example)\n",
        "!mkdir ~/testin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ce3LWg8l51-N"
      },
      "outputs": [],
      "source": [
        "# Copy example files to the input folder\n",
        "!cp /usr/local/hadoop-3.3.0/etc/hadoop/*.xml ~/testin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check that files have been successfully copied (10 files should appear)\n",
        "!ls ~/testin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the output folder to run Hadoop again\n",
        "!rm -r ~/testout\n",
        "# Run the mapreduce example (for sanity check)\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar \\\n",
        "    grep ~/testin ~/testout 'allowed[.]*'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "010SYzB0zX5V"
      },
      "source": [
        "# Problem 2\n",
        "## Word Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the data in CSV files (preprocessed news data)\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the data in CSV files (preprocessed training news data)\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the output folder to run Hadoop again\n",
        "!rm -r ~/testout\n",
        "# Run hadoop to execute the mapper and reducer using the train.csv file\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
        "    -files /content/mapper.py -files /content/reducer.py \\\n",
        "    -input /content/train.csv -output ~/testout \\\n",
        "    -mapper 'python mapper.py' -reducer 'python reducer.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mLTPzgBxLAE"
      },
      "source": [
        "# Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /content/3_tfidf.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAmYlfUuGP5Z"
      },
      "source": [
        "# Problem 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /content/4_jm_smoothing.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIfh4zUHMhC-"
      },
      "source": [
        "# Problem 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /content/5_crawler.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the output folder to run Hadoop again\n",
        "!rm -r ~/testout\n",
        "# Run hadoop to execute the mapper and reducer using the crawler.csv file\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
        "    -files /content/mapper.py -files /content/reducer.py \\\n",
        "    -input /content/crawler.csv -output ~/testout \\\n",
        "    -mapper 'python mapper.py' -reducer 'python reducer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /content/5_tfidf.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do3kZsetqLCv"
      },
      "source": [
        "# Problem 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the output folder to run Hadoop again\n",
        "!rm -r ~/testout\n",
        "# Run hadoop to execute the mapper and reducer using the train.csv file\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
        "    -files /content/mapper.py -files /content/reducer.py \\\n",
        "    -input /content/train.csv -output ~/testout \\\n",
        "    -mapper 'python mapper.py' -reducer 'python reducer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /content/6_cv_k.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
